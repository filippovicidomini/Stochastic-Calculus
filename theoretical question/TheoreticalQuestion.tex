\documentclass[12pt,a4paper]{book}

% Pacchetti utili
\usepackage[utf8]{inputenc}  % Per la codifica in UTF-8
\usepackage[T1]{fontenc}     % Font encoding
\usepackage{lmodern}         % Font Latin Modern
\usepackage{amsmath, amssymb, amsthm, mathtools, bm}
%\usepackage{hyperref}        % Collegamenti ipertestuali ma con rettangolo rosso
\usepackage{enumitem}        % Miglior gestione degli elenchi
\usepackage{geometry}        % Controllo margini
%fancy
\usepackage{fancyhdr}        % Intestazioni e piè di pagina
\usepackage[hidelinks]{hyperref}% togli il contorno rosso nell'indice
\geometry{a4paper, margin=2.5cm}

% Intestazioni e piè di pagina
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Theoretical Questions Stochastic Calculus}
\fancyhead[R]{\thepage}
%\fancyfoot[C]{\textit{Confidential - Do Not Distribute}}

% Ambienti per teoremi e definizioni
\newtheorem{definition}{Definition}[chapter]
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{example}{Example}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\theoremstyle{remark}
\newtheorem{remark}{Remark}[chapter]

% Comandi personalizzati per probabilità e processi stocastici
\newcommand{\PP}{\mathbb{P}}          % probabilità
\newcommand{\EE}{\mathbb{E}}          % valore atteso
\newcommand{\QQ}{\mathbb{Q}}          % misura risk-neutral
\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\F}{\mathcal{F}}          % sigma-algebra
\newcommand{\Filtr}[1]{\{\mathcal{F}_{#1}\}} % filtrazione
\newcommand{\Var}{\mathrm{Var}}       % varianza
\newcommand{\Cov}{\mathrm{Cov}}       % covarianza
\newcommand{\indic}{\mathds{1}}       % funzione indicatrice
% comando Normal for normal distribution
\newcommand{\Normal}{\mathcal{N}}

% Altri comandi utili
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\ang}{\langle}{\rangle}
\newcommand{\law}{\stackrel{d}{=}}    % uguaglianza in distribuzione
\newcommand{\given}{\,\middle|\,} % condizionamento

% Differenziali e calcolo stocastico
\newcommand{\dd}{\mathrm{d}}
\newcommand{\dt}{\,\mathrm{d}t}
\newcommand{\dx}{\,\mathrm{d}x}
\newcommand{\dW}{\,\mathrm{d}W_t}
\newcommand{\Ito}{It\^o}
\newcommand{\BM}{Brownian motion}

\begin{document}

\begin{titlepage}
    \centering
    % Logo dell'università (se vuoi inserirlo, basta caricare l'immagine)
    % \includegraphics[width=0.25\textwidth]{logo.png}\par\vspace{1cm}
    
    {\scshape Università Ca' Foscari Venezia \par}
 
    \vspace{2cm}
    \vfill
    {\Huge\bfseries Theoretical Questions on Stochastic Calculus \par}
    \vspace{1.5cm}
    {\Large\itshape Exam Preparation Booklet\par}
    
    \vspace{2cm}
    {\Large Course: Stochastic Calculus for Finance \par}
    \vspace{0.5cm}
 
    
    \vfill
    
    {\large Student: Filippo Vicidomini \par}
    {\large Master’s Degree in Engineering Physics\par}
    
    \vfill
    
    % Data
    {\large Academic Year 2024--2025 \par}
    
\end{titlepage}

%indice
\tableofcontents



\newpage
\section{Question 1}
\textbf{Explain what an ODE is, and what it means for a function $x(t)$ to be a (classical) solution. 
State the theorems that you know and that establish: (a) existence of a solution; (b) local existence and uniqueness of a solution; (c) global existence and uniqueness of a solution. 
Compare them and explain, give suitable examples.}

\subsection*{Answer}

\subsubsection*{Definition of ODE}
An \textbf{ordinary differential equation (ODE)} is an equation where the unknown is a function $x(t)$ of one real variable (often time $t$), and the function appears together with its derivatives.  
A general first–order ODE can be written in normal form as
\[
x'(t) = f(t,x(t)), \quad t \geq t_0.
\]

\subsubsection*{Classical Solution}
A function $x:[t_0,T]\to\RR$ is a \textbf{classical solution} to the Cauchy problem
\[
\begin{cases}
x'(t) = f(t,x(t)), \quad t \geq t_0, \\
x(t_0) = x_0,
\end{cases}
\]
if:
\begin{enumerate}[label=\roman*)]
    \item $x$ is differentiable on $[t_0,T]$;
    \item it satisfies $x'(t) = f(t,x(t))$ for all $t \in [t_0,T]$;
    \item it satisfies the initial condition $x(t_0) = x_0$.
\end{enumerate}

\subsubsection*{Theorems of Existence and Uniqueness}
\begin{itemize}
    \item \textbf{Peano’s Existence Theorem:} If $f(t,x)$ is continuous in a neighborhood of $(t_0,x_0)$, then there exists at least one solution $x(t)$ to the Cauchy problem on some interval around $t_0$. (Existence, but not uniqueness).
    
    \item \textbf{Picard–Lindelöf (Cauchy–Lipschitz) Theorem:} If $f(t,x)$ is continuous in $t$ and \emph{Lipschitz} continuous in $x$, then there exists a \emph{unique} local solution to the Cauchy problem. 
    
    \item \textbf{Global Existence and Uniqueness:} If the assumptions above hold on the entire domain (e.g. $f$ is globally Lipschitz or satisfies suitable growth conditions preventing blow–up), then the solution can be extended uniquely to all $t \geq t_0$.
\end{itemize}

\subsubsection*{Comparison}
\begin{itemize}
    \item Peano theorem ensures existence but possibly many solutions.
    \item Picard–Lindelöf ensures both existence and uniqueness, but only locally in time.
    \item Global results require additional conditions to extend the solution to all times.
\end{itemize}

\subsubsection*{Examples}
\begin{itemize}
    \item \textbf{Non-uniqueness (Peano):} 
    \[
    x'(t) = \sqrt{x(t)}, \quad x(0)=0.
    \] 
    Both $x(t)\equiv 0$ and $x(t) = \tfrac{t^2}{4}$ are solutions, showing non-uniqueness since $f(x)=\sqrt{x}$ is not Lipschitz at $0$.

    \item \textbf{Unique local (and global) solution (Picard–Lindelöf):} 
    \[
    x'(t) = t+x, \quad x(0)=1.
    \]
    Here $f(t,x)=t+x$ is Lipschitz in $x$. The unique solution is
    \[
    x(t) = Ce^t - t - 1.
    \]

    \item \textbf{Global existence:} 
    \[
    x'(t) = -x(t), \quad x(0)=x_0.
    \]
    Solution: $x(t)=x_0e^{-t}$, which exists uniquely for all $t\geq 0$.
\end{itemize}













\newpage
\section{Question 2}
\textbf{Say what a linear ODE is. Show that, under suitable assumptions on the coefficients (exemplify), a Cauchy problem for a linear ODE has a unique solution. Prove that such solution is given by the known solution formula.}

\subsection*{Answer}

\subsubsection*{Definition}
A \textbf{first–order linear ODE} is an equation of the form
\[
x'(t) = a(t)x(t) + b(t), \quad t \ge t_0,
\]
where $a,b:I\to\RR$ are given real functions on an interval $I\ni t_0$. The associated \textbf{Cauchy problem} is
\[
\begin{cases}
x'(t) = a(t)x(t) + b(t),\\
x(t_0)=x_0.
\end{cases}
\]

\subsubsection*{Existence and Uniqueness (Picard–Lindel\"of)}
If $a(\cdot)$ and $b(\cdot)$ are continuous on $I$, then $f(t,x)=a(t)x+b(t)$ is continuous and Lipschitz in $x$ on compatti: 
\[
|f(t,x)-f(t,y)|=|a(t)||x-y| \le L|x-y| \quad (t\in K\Subset I).
\]
Hence the Cauchy problem admits a \textbf{unique local} solution; if $a,b$ are continuous on all of $I$ and no blow-up occurs, the solution extends \textbf{uniquely} on $I$.

\subsubsection*{Derivation (Proof via integrating factor)}
Consider
\[
x'(t)-a(t)x(t)=b(t).
\]
Let the integrating factor be
\[
\mu(t)=\exp\!\Big(\int_{t_0}^t a(s)\,\dd s\Big),\qquad \mu(t)>0,\;\mu'(t)=a(t)\mu(t).
\]
Multiply the ODE by $\mu(t)$:
\[
\mu(t)x'(t)-\mu(t)a(t)x(t)=\mu(t)b(t).
\]
By the product rule,
\[
\frac{\dd}{\dd t}\big(\mu(t)x(t)\big)=\mu(t)b(t).
\]
Integrate from $t_0$ to $t$:
\[
\mu(t)x(t)-\mu(t_0)x(t_0)=\int_{t_0}^t \mu(r)b(r)\,\dd r.
\]
Since $\mu(t_0)=1$, $x(t_0)=x_0$, we obtain the \textbf{solution formula}
\[
\boxed{\;
x(t)=\exp\!\Big(\int_{t_0}^t a(s)\,\dd s\Big)\left[
x_0+\int_{t_0}^t \exp\!\Big(-\int_{t_0}^r a(u)\,\dd u\Big)\,b(r)\,\dd r
\right].\;}
\]

\subsubsection*{Verification (Plug-in)}
Set $\mu(t)=\exp\!\big(\int_{t_0}^t a\big)$ and write
\[
x(t)=\mu(t)\Big(x_0+\int_{t_0}^t \mu(r)^{-1}b(r)\,\dd r\Big).
\]
Differentiate:
\[
x'(t)=\mu'(t)\Big(x_0+\int_{t_0}^t \mu(r)^{-1}b(r)\,\dd r\Big)+\mu(t)\cdot \mu(t)^{-1}b(t).
\]
Since $\mu'(t)=a(t)\mu(t)$, we get
\[
x'(t)=a(t)\mu(t)\Big(\cdots\Big)+b(t)=a(t)x(t)+b(t),
\]
quindi $x$ soddisfa l’ODE. Inoltre $x(t_0)=\mu(t_0)\big(x_0+0\big)=x_0$. Per unicità (Picard–Lindel\"of), questa è \emph{la} soluzione del problema di Cauchy.

\subsubsection*{Example}
Let
\[
x'(t)=2x(t)+t,\qquad x(0)=1.
\]
Here $a(t)=2$, $b(t)=t$, $\mu(t)=e^{2t}$. Thus
\[
x(t)=e^{2t}\left(1+\int_0^t e^{-2r}\,r\,\dd r\right)
= e^{2t}\left(1-\tfrac12 t e^{-2t}-\tfrac14 e^{-2t}+\tfrac14\right).
\]
This (unique) solution is defined for all $t\in\RR$.













\newpage
\section{Question 6}
\textbf{State and interpret the definition of:  
a) sigma algebra;  
b) sigma algebra generated by a random variable;  
c) filtration;  
d) stochastic process;  
e) stochastic process adapted to a filtration.}

\subsection*{Answer}

\subsubsection*{a) Sigma algebra}
Let $\Omega$ be a sample space. A \textbf{$\sigma$-algebra} $\F$ on $\Omega$ is a collection of subsets of $\Omega$ such that:
\begin{enumerate}[label=\roman*)]
    \item $\Omega \in \F$;
    \item If $A \in \F$, then $A^c \in \F$ (closed under complementation);
    \item If $\{A_n\}_{n=1}^\infty \subseteq \F$, then $\bigcup_{n=1}^\infty A_n \in \F$ (closed under countable unions).
\end{enumerate}
By De Morgan’s laws, $\F$ is also closed under countable intersections.  

\textbf{Interpretation:} a $\sigma$-algebra represents the collection of events that can be “observed” or “measured” in a probabilistic experiment.  

\textbf{Example:} On $\RR$, the Borel $\sigma$-algebra $\mathcal{B}(\RR)$ is generated by all open intervals $(a,b)$.

---

\subsubsection*{b) Sigma algebra generated by a random variable}
Given a random variable $X:\Omega \to \RR$, the \textbf{$\sigma$-algebra generated by $X$}, denoted $\sigma(X)$, is the smallest $\sigma$-algebra such that $X$ is measurable. Formally,
\[
\sigma(X) = \{ X^{-1}(B) : B \in \mathcal{B}(\RR)\}.
\]

\textbf{Interpretation:} $\sigma(X)$ contains exactly the events that can be described in terms of the knowledge of $X$.

\textbf{Example:} If $X(\omega)=1$ when a coin toss is Head and $0$ otherwise, then $\sigma(X)=\{\emptyset, \Omega, \{X=1\}, \{X=0\}\}$.

---

\subsubsection*{c) Filtration}
A \textbf{filtration} $\{\F_t\}_{t\ge0}$ is an increasing family of $\sigma$-algebras:
\[
\F_s \subseteq \F_t \quad \text{for all } 0 \leq s \leq t.
\]

\textbf{Interpretation:} $\F_t$ represents the information available up to time $t$. As time progresses, information increases.  

\textbf{Example:} For a Brownian motion $W(t)$, the \emph{natural filtration} is $\F_t=\sigma(W(s):0\leq s \leq t)$, i.e. all events determined by the past trajectory of $W$ up to time $t$.

---

\subsubsection*{d) Stochastic process}
A \textbf{stochastic process} is a family $\{X(t)\}_{t\ge0}$ of random variables defined on a common probability space $(\Omega, \F, \PP)$.  

\textbf{Interpretation:} $X(t)$ describes the random evolution of a system in time.  
- Fixing $t$, $X(t)$ is a random variable on $\Omega$.  
- Fixing $\omega \in \Omega$, $t \mapsto X(t,\omega)$ is a trajectory (sample path).

\textbf{Example:} A random walk $M_n=\sum_{j=1}^n X_j$ with i.i.d. $\pm1$ steps is a stochastic process indexed by $n\in\NN$.

---

\subsubsection*{e) Adapted stochastic process}
A stochastic process $\{X(t)\}_{t\ge0}$ is \textbf{adapted} to a filtration $\{\F_t\}$ if, for each $t$, $X(t)$ is $\F_t$-measurable.

\textbf{Interpretation:} At time $t$, the value $X(t)$ depends only on the information available up to $t$, not on the future.  

\textbf{Example:} The Brownian motion $W(t)$ is adapted to its natural filtration $\{\F_t\}$, since $W(t)$ is $\F_t$-measurable by construction.













\newpage
\section{Question 7}
\textbf{Give the definition of the martingale property for a stochastic process and interpret it. Give suitable examples of stochastic processes with this property.}

\subsection*{Answer}
Let $(\Omega, \F, \PP)$ be a probability space and $\Filtr{t}$ a filtration. A stochastic process $X(t)$ adapted to $\Filtr{t}$ is called a \textbf{martingale} if:

\begin{enumerate}[label=\roman*)]
    \item $\EE[\abs{X(t)}] < \infty$ for all $t$;
    \item For all $s < t$, 
    \[
        \EE[X(t) \mid \F_s] = X(s).
    \]
\end{enumerate}

\subsection*{Interpretation}
A martingale represents a \textbf{fair game}: given the information available up to time $s$, the best prediction of the value at time $t$ is exactly the current value $X(s)$. This means the process has no drift: it does not systematically increase or decrease.

Consequently, $\EE[X(t)] = \EE[X(0)]$ for all $t$.

\subsection*{Examples}
\begin{itemize}
    \item \textbf{Symmetric random walk}: $M_n = \sum_{j=1}^n X_j$ with $X_j = \pm 1$ with equal probability, is a martingale with respect to the natural filtration.
    \item \textbf{Brownian motion} $W(t)$: is a martingale with respect to its natural filtration.
    \item \textbf{It\^o integrals}: if $\Delta(t)$ is adapted and square-integrable, then 
    \[
    I(t) = \int_0^t \Delta(s)\,\dd W(s)
    \]
    is a martingale with zero mean.
\end{itemize}

\subsection*{Counterexample}
A Geometric Brownian Motion 
\[
S(t) = S(0) e^{(\alpha - \tfrac{1}{2}\sigma^2)t + \sigma W(t)}
\]
is not a martingale if $\alpha \neq 0$, since it has exponential drift. However, under the risk-neutral measure $\QQ$, the discounted price $e^{-rt}S(t)$ is a martingale. This property is fundamental in financial mathematics (e.g., Black--Scholes model).


\newpage
\section{Question 8}
\textbf{Describe the construction of a Brownian motion.}

\subsection*{Answer}
A Brownian motion, also known as a Wiener process, is a stochastic process $W(t)$ defined on a probability space $(\Omega, \F, \PP)$ that satisfies the following properties:

\begin{enumerate}[label=\roman*)]
    \item $W(0) = 0$ almost surely;
    \item $W(t)$ has independent increments: for $0 \leq t_0 < t_1 < \cdots < t_n$, the increments 
    \[
        W(t_1) - W(t_0), \; W(t_2) - W(t_1), \ldots, W(t_n) - W(t_{n-1})
    \]
    are independent random variables;
    \item $W(t)$ has Gaussian increments: for $s < t$, the increment $W(t) - W(s)$ is normally distributed with mean $0$ and variance $t-s$;
    \item $W(t)$ has continuous trajectories almost surely.
\end{enumerate}

\subsection*{Construction via Random Walks}
One can construct a Brownian motion as the limit of suitably rescaled symmetric random walks:

\begin{itemize}
    \item Consider a sequence $(X_j)_{j\geq 1}$ of i.i.d. random variables with
    \[
        \PP(X_j = 1) = \PP(X_j = -1) = \tfrac{1}{2}.
    \]
    \item Define the partial sums (a symmetric random walk):
    \[
        M_k = \sum_{j=1}^k X_j, \quad M_0 = 0.
    \]
    Then $\EE[M_k] = 0$, $\Var(M_k) = k$.
    \item Define the scaled random walk:
    \[
        W^{(n)}(t) = \frac{1}{\sqrt{n}} M_{\lfloor nt \rfloor}, \quad t \geq 0.
    \]
    \item As $n \to \infty$, the processes $W^{(n)}(t)$ converge in distribution to a process $W(t)$ that satisfies the above four properties.
\end{itemize}

The limit process $W(t)$ is called a \textbf{Brownian motion}.

\subsection*{Properties}
From this construction, Brownian motion inherits:
\begin{itemize}
    \item Mean zero: $\EE[W(t)] = 0$;
    \item Variance linear in time: $\Var(W(t)) = t$;
    \item Independent, Gaussian increments;
    \item Quadratic variation: $[W,W]_t = t$;
    \item Martingale property: $\EE[W(t) \mid \F_s] = W(s)$ for $s < t$.
\end{itemize}

\subsection*{Interpretation}
Brownian motion models continuous-time randomness:
\begin{itemize}
    \item In physics, it describes the irregular motion of particles suspended in a fluid.
    \item In finance, it underlies models of asset price fluctuations (e.g., geometric Brownian motion in the Black--Scholes framework).
\end{itemize}










\newpage
\section{Question 9}
\textbf{Describe the construction of a random walk and of a scaled random walk. Show that a Brownian motion can be obtained as a limit of scaled random walks.}

\subsection*{Answer}

\subsubsection*{Random Walk}
Let $\{X_j\}_{j\geq 1}$ be a sequence of i.i.d. random variables with
\[
\PP(X_j = 1) = \PP(X_j = -1) = \tfrac{1}{2}.
\]
Define the partial sums
\[
M_k = \sum_{j=1}^k X_j, \quad M_0=0.
\]
The process $\{M_k\}_{k\in\NN}$ is called a \textbf{symmetric random walk}.  
Properties:
\begin{itemize}
    \item $\EE[M_k] = 0$, $\Var(M_k) = k$.
    \item Increments are independent and stationary: $M_{n+m}-M_n \sim \Normal(0,m)$.
    \item $M_k$ is a martingale with respect to the natural filtration.
\end{itemize}

\subsubsection*{Scaled Random Walk}
To approach a continuous-time process, rescale both time and space:
\[
W^{(n)}(t) = \frac{1}{\sqrt{n}} M_{\lfloor nt \rfloor}, \quad t \ge 0.
\]
Interpretation:
\begin{itemize}
    \item Time is accelerated by factor $n$ (steps of size $1/n$).
    \item Space is scaled down by $1/\sqrt{n}$ (variance normalisation).
\end{itemize}
Thus $W^{(n)}(t)$ is a piecewise constant, right–continuous process with jumps $\pm 1/\sqrt{n}$ at times $k/n$.

\subsubsection*{Limit Process: Brownian Motion}
By Donsker’s invariance principle (or functional Central Limit Theorem),
\[
W^{(n)}(t) \;\; \xrightarrow{d}\;\; W(t), \quad \text{as } n\to\infty,
\]
where $W(t)$ is a \textbf{Brownian motion}.  

\textbf{Proof idea:}
\begin{itemize}
    \item Finite-dimensional distributions: by the Central Limit Theorem, for fixed $t$, 
    \[
    W^{(n)}(t) = \frac{1}{\sqrt{n}}M_{\lfloor nt \rfloor} \;\law\; \Normal(0,t).
    \]
    \item Independence of increments: inherited from independence of $X_j$.
    \item Continuous trajectories: obtained in the limit (the $W^{(n)}$ are piecewise constant, but converge in distribution to a continuous process).
\end{itemize}

\subsubsection*{Conclusion}
A Brownian motion $W(t)$ is obtained as the scaling limit of a symmetric random walk.  
\[
W(t) = \lim_{n\to\infty} W^{(n)}(t) \quad \text{in distribution}.
\]

\subsection*{Example}
Simulating many paths of a scaled random walk with large $n$, the trajectories approximate continuous Brownian paths with variance $t$ and independent Gaussian increments.








\newpage
\section{Question 10}
\textbf{List the properties of a Brownian Motion. Comment on consequences of (at least two of) such properties, giving examples of applications.}

\subsection*{Answer}

\subsubsection*{Definition}
A \textbf{Brownian motion} (or Wiener process) $\{W(t)\}_{t\ge 0}$ on a probability space $(\Omega,\F,\PP)$ with filtration $\{\F_t\}$ is a stochastic process such that:

\begin{enumerate}[label=\roman*)]
    \item $W(0)=0$ almost surely;
    \item Independent increments: for $0\le t_0 < t_1 < \cdots < t_n$, the increments $W(t_j)-W(t_{j-1})$ are independent;
    \item Stationary Gaussian increments: for $s<t$, 
    \[
    W(t)-W(s) \sim \Normal(0,t-s);
    \]
    \item Almost surely continuous trajectories $t\mapsto W(t,\omega)$;
    \item Quadratic variation: $[W,W]_t = t$;
    \item $\{W(t)\}$ is a martingale with respect to $\{\F_t\}$.
\end{enumerate}

\subsubsection*{Consequences and Applications}

\paragraph{1. Independent and Gaussian increments.}
This property implies that the process has the \emph{Markov property}: the future evolution depends only on the present, not the past.  
\emph{Application:} In finance, this justifies the modeling of asset prices as functions of Brownian motion (e.g. geometric Brownian motion). The independence of increments makes simulation and option pricing tractable.

\paragraph{2. Continuity and nowhere differentiability.}
Brownian paths are continuous but almost surely nowhere differentiable. This prevents interpreting $dW/dt$ in the classical sense, motivating the development of Itô calculus.  
\emph{Application:} In physics, this models erratic particle trajectories (Einstein’s model of molecular diffusion). In finance, it justifies stochastic differentials like
\[
dS(t) = \mu S(t)\,dt + \sigma S(t)\,dW(t).
\]

\paragraph{3. Quadratic variation $[W]_t = t$.}
This fundamental property distinguishes Brownian motion from deterministic differentiable functions. It leads to the extra $\tfrac{1}{2}f''$ term in Itô’s formula.  
\emph{Application:} Pricing via Black–Scholes model relies on Itô’s formula, where the quadratic variation produces the diffusion term in the PDE.

\paragraph{4. Martingale property.}
Since $\EE[W(t)\mid\F_s]=W(s)$, the Brownian motion is a martingale.  
\emph{Application:} In risk–neutral valuation, discounted asset prices must be martingales under the risk–neutral measure. Brownian motion is the core driving noise ensuring absence of arbitrage.

\subsubsection*{Summary}
Brownian motion is the canonical continuous–time stochastic process, whose properties (independent Gaussian increments, continuity, quadratic variation, martingale property) make it the fundamental building block for stochastic calculus, diffusion models, and modern financial mathematics.




\newpage
\section{Question 12}
\textbf{Give the definition of a diffusion process. Explain the necessity of an Itô Calculus for the study of evolution of a stochastic process.}

\subsection*{Answer}

\subsubsection*{Definition of a Diffusion Process}
A \textbf{diffusion process} $\{X(t)\}_{t\ge 0}$ is a continuous-time stochastic process defined as the solution of a stochastic differential equation (SDE) of the form
\[
dX(t) = \mu(t,X(t))\,dt + \sigma(t,X(t))\,dW(t), \quad X(0)=x_0,
\]
where:
\begin{itemize}
    \item $W(t)$ is a Brownian motion;
    \item $\mu(t,x)$ is the \emph{drift coefficient}, governing the deterministic trend;
    \item $\sigma(t,x)$ is the \emph{diffusion coefficient}, scaling the random noise.
\end{itemize}
Thus, a diffusion is a continuous Markov process whose local dynamics are described by a drift and a diffusion term.

\subsubsection*{Necessity of Itô Calculus}
Ordinary calculus is not sufficient to study stochastic processes like diffusions because:
\begin{enumerate}[label=\roman*)]
    \item Paths of Brownian motion are almost surely continuous but nowhere differentiable, so $W'(t)$ does not exist in the classical sense;
    \item Quadratic variation of Brownian motion is nonzero: $[W]_t = t$. This breaks the rules of standard calculus (where higher-order terms vanish).
\end{enumerate}

\textbf{Itô Calculus} provides:
\begin{itemize}
    \item A rigorous definition of the stochastic integral
    \[
    \int_0^t \sigma(s,X(s))\,dW(s),
    \]
    for adapted, square-integrable processes $\sigma$;
    \item The \textbf{Itô formula}, a stochastic analogue of the chain rule, which includes an extra term due to quadratic variation:
    \[
    df(X(t)) = f_x(X(t))\,dX(t) + \tfrac{1}{2} f_{xx}(X(t))\,\sigma^2(t,X(t))\,dt.
    \]
\end{itemize}

\subsubsection*{Applications}
\begin{itemize}
    \item In \textbf{physics}, diffusions describe random particle motion (Einstein’s model of molecular diffusion).
    \item In \textbf{finance}, asset prices are modeled as diffusions (e.g. geometric Brownian motion), and Itô calculus underpins the derivation of option pricing models like Black–Scholes.
\end{itemize}

\subsubsection*{Conclusion}
Diffusion processes generalize deterministic dynamical systems by adding stochastic noise. Their study requires Itô calculus, since classical tools of analysis are not valid for processes driven by Brownian motion.

A \textbf{diffusion process} $\{X(t)\}_{t\ge 0}$ is a continuous-time stochastic process defined as the solution of a stochastic differential equation (SDE) of the form
\[
dX(t) = \mu(t,X(t))\,dt + \sigma(t,X(t))\,dW(t), \quad X(0)=x_0,
\]
where:
\begin{itemize}
    \item $W(t)$ is a Brownian motion;
    \item $\mu(t,x)$ is the \emph{drift coefficient}, governing the deterministic trend;
    \item $\sigma(t,x)$ is the \emph{diffusion coefficient}, scaling the random noise.
\end{itemize}
Thus, a diffusion is a continuous Markov process whose local dynamics are described by a drift and a diffusion term.

\subsubsection*{Necessity of Itô Calculus}
Ordinary calculus is not sufficient to study stochastic processes like diffusions because:
\begin{enumerate}[label=\roman*)]
    \item Paths of Brownian motion are almost surely continuous but nowhere differentiable, so $W'(t)$ does not exist in the classical sense;
    \item Quadratic variation of Brownian motion is nonzero: $[W]_t = t$. This breaks the rules of standard calculus (where higher-order terms vanish).
\end{enumerate}

\textbf{Itô Calculus} provides:
\begin{itemize}
    \item A rigorous definition of the stochastic integral
    \[
    \int_0^t \sigma(s,X(s))\,dW(s),
    \]
    for adapted, square-integrable processes $\sigma$;
    \item The \textbf{Itô formula}, a stochastic analogue of the chain rule, which includes an extra term due to quadratic variation:
    \[
    df(X(t)) = f_x(X(t))\,dX(t) + \tfrac{1}{2} f_{xx}(X(t))\,\sigma^2(t,X(t))\,dt.
    \]
\end{itemize}

\subsubsection*{Applications}
\begin{itemize}
    \item In \textbf{physics}, diffusions describe random particle motion (Einstein’s model of molecular diffusion).
    \item In \textbf{finance}, asset prices are modeled as diffusions (e.g. geometric Brownian motion), and Itô calculus underpins the derivation of option pricing models like Black–Scholes.
\end{itemize}

\subsubsection*{Conclusion}
Diffusion processes generalize deterministic dynamical systems by adding stochastic noise. Their study requires Itô calculus, since classical tools of analysis are not valid for processes driven by Brownian motion.


\newpage
\section{Question 13}
\textbf{Describe the construction of the Itô integral for a stochastic process.}

\subsection*{Answer}

\subsubsection*{Aim}
We want to give a rigorous meaning to the stochastic integral
\[
I(t) = \int_0^t \Delta(s)\,dW(s),
\]
where $W(t)$ is a Brownian motion and $\Delta(s)$ is a stochastic process adapted to the filtration $\{\F_s\}$.

---

\subsubsection*{Step 1: Constant integrands}
For a constant process $\Delta(s) \equiv C$, define
\[
\int_0^T C\,dW(s) := C\,(W(T)-W(0)).
\]

---

\subsubsection*{Step 2: Simple processes}
For a simple (piecewise constant, adapted) process
\[
\Delta(s) = \sum_{j=0}^{n-1} c_j \,\chi_{[t_j,t_{j+1})}(s), \quad 0=t_0<\dots<t_n=T,
\]
define
\[
\int_0^T \Delta(s)\,dW(s) := \sum_{j=0}^{n-1} c_j \big(W(t_{j+1})-W(t_j)\big).
\]

---

\subsubsection*{Step 3: General processes}
If $\Delta(s)$ is progressively measurable and square-integrable, i.e.
\[
\EE\!\left[\int_0^T \Delta(s)^2 \,ds\right] < \infty,
\]
then we approximate $\Delta(s)$ by a sequence of simple processes $\Delta^n(s)$ in $L^2([0,T]\times\Omega)$.  
The Itô integral is defined as the $L^2$–limit:
\[
\int_0^T \Delta(s)\,dW(s) := \lim_{n\to\infty} \int_0^T \Delta^n(s)\,dW(s).
\]

---

\subsubsection*{Properties}
The Itô integral $I(t)=\int_0^t \Delta(s)\,dW(s)$ satisfies:
\begin{itemize}
    \item \textbf{Linearity:} $\int (a\Delta_1+b\Delta_2)\,dW = a\int\Delta_1\,dW + b\int\Delta_2\,dW$;
    \item \textbf{Isometry (Itô isometry):}
    \[
    \EE\!\left[\left(\int_0^T \Delta(s)\,dW(s)\right)^2\right] = \EE\!\left[\int_0^T \Delta^2(s)\,ds\right];
    \]
    \item $\EE\!\big[\int_0^T \Delta(s)\,dW(s)\big] = 0$ (zero mean);
    \item $I(t)$ is a martingale with respect to $\{\F_t\}$.
\end{itemize}

---

\subsubsection*{Interpretation}
The Itô integral extends the notion of integration to stochastic processes.  
- The approximation by simple processes reflects the idea of integrating “stepwise predictable strategies” against Brownian motion.  
- It is fundamental in defining stochastic differential equations and in deriving Itô’s formula.  
- In finance, it represents the gains from trading strategies where $\Delta(t)$ is the number of risky assets held at time $t$.







\newpage
\section{Question 14}
\textbf{List the properties of the It\^o integral. Give an interpretation, or comment, at least of some properties. Show one significant application.}

\subsection*{Answer}

Let $I(t)=\int_0^t \Delta(s)\,\dd W(s)$ with $\Delta$ progressively measurable and $\EE\!\big[\int_0^T \Delta^2(s)\,\dd s\big]<\infty$.

\subsubsection*{Main properties}
\begin{enumerate}[label=\roman*)]
\item \textbf{Linearity:} $\displaystyle \int_0^t (a\Delta_1+b\Delta_2)\,\dd W = a\int_0^t \Delta_1\,\dd W + b\int_0^t \Delta_2\,\dd W.$
\item \textbf{Isometria di It\^o:} $\displaystyle \EE\!\left[\left(\int_0^t \Delta\,\dd W\right)^2\right]=\EE\!\left[\int_0^t \Delta^2\,\dd s\right].$
\item \textbf{Zero mean:} $\displaystyle \EE\!\left[\int_0^t \Delta\,\dd W\right]=0.$
\item \textbf{Martingala:} $I(t)$ è una martingala w.r.t. $\{\F_t\}$ e ha traiettorie continue.
\item \textbf{Covarianza/Prodotto scalare:} per $\Delta,\Gamma\in L^2_{\text{prog}}$,
\[
\EE\!\left[\left(\int_0^t \Delta\,\dd W\right)\!\left(\int_0^t \Gamma\,\dd W\right)\right]
=\EE\!\left[\int_0^t \Delta(s)\Gamma(s)\,\dd s\right].
\]
\item \textbf{Variazione quadratica:} $\displaystyle \big[\textstyle\int_0^\cdot \Delta\,\dd W\big]_t=\int_0^t \Delta^2(s)\,\dd s.$
\item \textbf{Continuit\`a in $L^2$:} se $\Delta_n\to\Delta$ in $L^2([0,t]\times\Omega)$, allora $\int_0^t \Delta_n\,\dd W \to \int_0^t \Delta\,\dd W$ in $L^2(\Omega)$.
\end{enumerate}

\subsubsection*{Interpretazioni/Commenti}
\begin{itemize}
\item (iii) \emph{Zero mean} $\Rightarrow$ il guadagno stocastico “puro” ha valore atteso nullo: gioco equo dato il passato.
\item (ii)+(v) \emph{Isometria come isometria di Hilbert}: l’It\^o integrale realizza un’isometria $L^2_{\text{prog}} \to \mathcal{M}^2$ (martingale square–integrable); utile per proiezioni/ortogonalit\`a.
\item (vi) La \emph{variazione quadratica} determina il termine $\tfrac12 f''$ nella formula di It\^o: base dell’analisi di SDE e delle PDE associate.
\end{itemize}

\subsubsection*{Applicazione significativa (pricing risk–neutral)}
Sia $S$ soluzione di $\,\dd S_t = r S_t\,\dd t + \sigma(t) S_t\,\dd W_t.$ Allora
\[
\dd\big(e^{-rt}S_t\big)= e^{-rt}\sigma(t)S_t\,\dd W_t
\quad\Rightarrow\quad
e^{-rt}S_t = S_0 + \int_0^t e^{-rs}\sigma(s)S_s\,\dd W_s.
\]
Per (iii) e (iv): $\EE[e^{-rt}S_t]=S_0$ e $(e^{-rt}S_t)$ è martingala $\Rightarrow$ \emph{assenza d’arbitraggio} e base della valutazione risk–neutral.









\newpage
\section{Question 15}
\textbf{If $I(t)$ is an It\^o integral, what is $\EE(I(t))$? Show one significant application of the property.}

\subsection*{Answer}

\subsubsection*{Statement}
If $I(t)=\displaystyle \int_0^t \Delta(s)\,\dd W(s)$ with $\Delta$ adattato e $\EE\!\big[\int_0^t \Delta^2\,\dd s\big]<\infty$, allora
\[
\boxed{\;\EE[I(t)]=0\;}\quad \text{per ogni } t\ge 0.
\]

\subsubsection*{Reason}
Per definizione via approssimazione con processi semplici (somma di incrementi di $W$ a media zero) e passaggio al limite in $L^2$.

\subsubsection*{Applicazione significativa (media delle soluzioni SDE lineari)}
Considera la SDE lineare (tipo Vasicek con coefficiente generico)
\[
\dd X_t = a(t)X_t\,\dd t + b(t)\,\dd t + \sigma(t)\,\dd W_t,\qquad X_0=x_0.
\]
La soluzione var.\ dei parametri \`e
\[
X_t = \Phi(t)\!\left(x_0 + \int_0^t \Phi(s)^{-1} b(s)\,\dd s\right)
+ \underbrace{\int_0^t \Phi(t)\Phi(s)^{-1}\sigma(s)\,\dd W_s}_{\text{It\^o integrale, media 0}},
\]
dove $\Phi'(t)=a(t)\Phi(t)$, $\Phi(0)=1$. Quindi
\[
\EE[X_t]= \Phi(t)\!\left(x_0 + \int_0^t \Phi(s)^{-1} b(s)\,\dd s\right),
\]
ovvero \emph{il termine diffusivo non contribuisce alla media}. Questo \`e cruciale, ad es., per $\EE[S_t]$ in GBM e per $\EE[r_t]$ nei modelli di tasso.




\newpage
\section{Question 16}
\textbf{State the isometry property of the It\^o integral. Show an example of application. (hint: compute variance of either the Vasicek or the CIR interest rate)}

\subsection*{Answer}

\subsubsection*{It\^o Isometry}
For $\Delta \in L^2_{\text{prog}}([0,t]\times\Omega)$,
\[
\boxed{\;\EE\!\left[\left(\int_0^t \Delta(s)\,\dd W(s)\right)^{\!2}\right]
= \EE\!\left[\int_0^t \Delta^2(s)\,\dd s\right].\;}
\]
Pi\`u in generale, per $\Delta,\Gamma$,
\[
\EE\!\left[\!\left(\int_0^t \Delta\,\dd W\right)\!\!\left(\int_0^t \Gamma\,\dd W\right)\!\right]
= \EE\!\left[\int_0^t \Delta(s)\Gamma(s)\,\dd s\right].
\]

\subsubsection*{Applicazione: varianza nel modello di Vasicek}
Sia il tasso cort o $r_t$ soluzione di
\[
\dd r_t = a\,(b-r_t)\,\dd t + \sigma\,\dd W_t,\qquad r_0 \in\RR,\; a>0,\; \sigma>0.
\]
La soluzione esplicita \`e
\[
r_t = r_0 e^{-at} + b\big(1-e^{-at}\big) + \sigma \int_0^t e^{-a(t-s)}\,\dd W_s.
\]
Allora
\[
\EE[r_t] = r_0 e^{-at} + b\big(1-e^{-at}\big),
\qquad
\Var(r_t) = \EE\!\left[\left(\sigma \int_0^t e^{-a(t-s)}\,\dd W_s\right)^{\!2}\right].
\]
Per isometria di It\^o,
\[
\Var(r_t) = \sigma^2 \int_0^t e^{-2a(t-s)}\,\dd s
= \frac{\sigma^2}{2a}\Big(1-e^{-2at}\Big).
\]
\textit{Commento.} La varianza cresce a $t\!\to\!\infty$ verso $\sigma^2/(2a)$: mean reversion ($a$ grande) riduce la varianza di lungo periodo.

% (Opzionale) Nota su CIR:
% Per CIR: \dd r_t = a(b-r_t)\dd t + \sigma \sqrt{r_t}\dd W_t, la varianza si ottiene via equazioni per i momenti o Feller; non lineare, ma l'isometria resta centrale nel trattare i termini stocastici.

\newpage
\section{Question 18}
\textbf{State the It\^o--Doeblin formula for the Brownian motion, both in integral and differential form. Compare it to the usual differentiation formula. Prove that the It\^o integral does not coincide with the usual Riemann, or Lebesgue, integral by means of suitable examples.}

\subsection*{Answer}

\subsubsection*{It\^o--Doeblin formula (Brownian motion)}
Let $W_t$ be a Brownian motion and $f\in C^2(\RR)$.

\paragraph{Differential form (time-homogeneous)}
\[
\boxed{\; \dd f(W_t) \;=\; f'(W_t)\,\dd W_t \;+\; \tfrac12\,f''(W_t)\,\dd t \;}
\]

\paragraph{Integral form}
\[
\boxed{\; f(W_t) \;=\; f(W_0) \;+\; \int_0^t f'(W_s)\,\dd W_s \;+\; \tfrac12\int_0^t f''(W_s)\,\dd s \;}
\]

\paragraph{Versione dipendente dal tempo $f\in C^{1,2}([0,T]\times\RR)$}
\[
\boxed{\; \dd f(t,W_t)= f_t(t,W_t)\,\dd t + f_x(t,W_t)\,\dd W_t + \tfrac12 f_{xx}(t,W_t)\,\dd t \;}
\]
\[
\boxed{\; f(t,W_t)= f(0,W_0)+ \int_0^t f_t(s,W_s)\,\dd s + \int_0^t f_x(s,W_s)\,\dd W_s + \tfrac12\int_0^t f_{xx}(s,W_s)\,\dd s \;}
\]

\medskip
\noindent\textit{It\^o table:} \; $\dd t\,\dd t=0,\; \dd t\,\dd W_t=0,\; (\dd W_t)^2=\dd t.$

\subsubsection*{Confronto con la regola di derivazione classica}
Per una funzione liscia $f$ e una curva deterministica $x(t)$: \; $\dd f(x(t)) = f'(x(t))\,\dd x(t)$.
\medskip

Per $x(t)=W_t$ la regola classica fallisce: compare l’ulteriore termine $\tfrac12 f''\,\dd t$ dovuto alla variazione quadratica $[W]_t=t$. Questo è il segno distintivo del calcolo di It\^o.

\subsubsection*{Perché l’integrale di It\^o non coincide con Riemann/Lebesgue: esempi}

\paragraph{Esempio A (catena su $f(x)=x^2$).}
Applicando It\^o a $f(x)=x^2$:
\[
\dd (W_t^2)= 2W_t\,\dd W_t + \dd t
\quad\Rightarrow\quad
\int_0^t W_s\,\dd W_s = \tfrac12\big(W_t^2 - t\big).
\]
Se $\int_0^t W_s\,\dd W_s$ fosse un integrale di Riemann--Stieltjes classico, per integrazione per parti avremmo
$\int_0^t W_s\,\dd W_s = \tfrac12 W_t^2$ (nessun termine $-\,\tfrac12 t$).
L’identità di It\^o mostra l’\emph{extra} $-\tfrac12 t$: i due integrali non coincidono.

\paragraph{Esempio B (inesistenza del Riemann--Stieltjes con integratore $W$).}
Il moto browniano ha variazione totale infinita su ogni intervallo e regolarità di Hölder $<\tfrac12$; l’integrale di Riemann--Stieltjes $\int_0^t W_s\,\dd W_s$ \emph{non esiste} pathwise (fallisce il criterio di Young). L’integrale di It\^o è definito invece come limite $L^2$ di somme prevedibili (left-point), quindi è ben definito e diverso dall’integrale classico.

\paragraph{Esempio C (It\^o vs Lebesgue nel tempo).}
Confronta
\[
I_t:=\int_0^t W_s\,\dd W_s
\qquad\text{e}\qquad
J_t:=\int_0^t W_s\,\dd s .
\]
Si hanno $\EE[I_t]=0$ e, per isometria di It\^o,
\[
\Var(I_t)= \EE\!\left[\int_0^t W_s^2\,\dd s\right]= \int_0^t \EE[W_s^2]\,\dd s = \int_0^t s\,\dd s = \tfrac{t^2}{2}.
\]
Invece
\[
\EE[J_t]=0,\qquad
\Var(J_t)= \iint_{[0,t]^2}\!\!\Cov(W_s,W_u)\,\dd s\,\dd u
= \iint_{[0,t]^2}\!\!\min(s,u)\,\dd s\,\dd u
= \tfrac{t^3}{3}.
\]
Dunque $I_t$ (It\^o, contro $W$) e $J_t$ (Lebesgue, contro $t$) sono variabili aleatorie diverse: l’integrale di It\^o non coincide con l’integrale di Lebesgue nel tempo.

\subsubsection*{Conclusione}
La formula di It\^o--Doeblin aggiunge un termine di drift $\tfrac12 f''\,\dd t$ assente nel calcolo classico: ciò riflette $[W]_t=t$ e rende necessario un \emph{calcolo} ad hoc. Gli esempi mostrano che l’integrale di It\^o è concettualmente e tecnicamente distinto dagli integrali di Riemann--Stieltjes e di Lebesgue.




\end{document}